\chapter{Some Infrastructure: Linear Algebra}  
\label{chap:linalg}   

RS methods, as with other machine learning (ML) techniques, often make
use of linear algebra, well beyond mere matrix multiplication.  Here we
will review/extend some issues of particular importance.

\section{Matrix Rank and Vector Linear Independence}

Consider the matrix

\begin{equation}
\label{rankex1}
M = 
\left (
\begin{array}{rrrr}
1 & 5 & 1 & -2 \\
8 & 3 & 2 & 8 \\
9 & 8 & 3 & 6 
\end{array}
\right )
\end{equation}

Note that the third row is the sum of the first two.  In many contexts,
this would imply that there are really only two ``independent'' rows in
$M$ in some sense related to the application.  

Denote the rows of $M$ by $r_i, ~ i = 1,2,3$.  Recall that we say they
are \textit{linearly independent} if it is not possible to find scalars
$a_i$, at least one of them nonzero, such that the \textit{linear
combination} $a_1 r_1 + a_2 r_2 + a_3 r_3$ is equal to 0.  In this case
$a_1 = a_2 = 1$ and $a_3 = -1$ gives us 0, so the rows of $M$ are
linearly dependent.

Recall that the \textit{rank} of a matrix is its maximal number of
linearly independent rows or columns.  The rank of $M$ above is 2.

The reason we say ``rows or columns'' above is that it can be shown that
the row rank and column rank are the same.  Note that the implies that
the rank of a matrix is less than or equal to the minimum of the number
of rows and columns, i.e.\ the rank of an $r \times s$ matrix is thus at
most $\min(r,s)$.  In the case of equality, we say the matrix has
\textit{full rank}.  A ratings matrix, such as $A$ in Section \ref{mf},
should be of full rank, since there presumably are no exact dependencies
among users or items.

Recall too the notion of the \textit{basis} of a vector space
$\mathcal{V}$.  It is a linearly independent set of vectors whose linear
combinations collectively form all of $\mathcal{V}$.  And the
\textit{dimension} of $\mathcal{V}$ is the number of vectors in a basis
(which can be shown to be the same for all bases).


Here  $r_1$ and
$r_2$ form a basis for the \textit{row space} of $M$.  Alternatively,
$r_1$ and $r_3$ also form a basis, as do $r_2$ and $r_3$.

\section{Partitioned Matrices}
\label{partmat}

It is often helpful to partition a matrix into \textit{blocks} (often
called {\it tiles} in the parallel computation community).

\subsection{How It Works}

Consider matrices $A$, $B$ and $C$,

\begin{equation}
A = 
\left (
\begin{array}{ccc}
1 & 5 & 12 \\
0 & 3 & 6 \\
4 & 8 & 2
\end{array}
\right )
\end{equation}

and

\begin{equation}
B = 
\left (
\begin{array}{ccc}
0 & 2 & 5 \\
0 & 9 & 10 \\
1 & 1 & 2
\end{array}
\right ), 
\end{equation}

so that

\begin{equation}
C = AB = 
\left (
\begin{array}{ccc}
12 & 59 & 79 \\
6 & 33 & 42 \\
2 & 82 & 104
\end{array}
\right ) .
\end{equation}

We could partition A as, say,

\begin{equation}
A =
\left (
\begin{array}{cc}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{array}
\right ) ,
\end{equation}

where

\begin{equation}
A_{11} =
\left (
\begin{array}{cc}
1 & 5 \\
0 & 3
\end{array}
\right )  ,
\end{equation}

\begin{equation}
A_{12} =
\left (
\begin{array}{cc}
12 \\
6 
\end{array}
\right ),
\end{equation}

\begin{equation}
A_{21} =
\left (
\begin{array}{cc}
4 & 8 
\end{array}
\right )  
\end{equation}

and

\begin{equation}
A_{22} =
\left (
\begin{array}{c}
2
\end{array}
\right ).
\end{equation}

Similarly we would partition B and C into blocks of a compatible size to A,

\begin{equation}
B =
\left (
\begin{array}{cc}
B_{11} & B_{12} \\
B_{21} & B_{22}
\end{array}
\right )
\end{equation}

\begin{equation}
C =
\left (
\begin{array}{cc}
C_{11} & C_{12} \\
C_{21} & C_{22}
\end{array}
\right ) ,
\end{equation}

so that for example

\begin{equation}
B_{21} =
\left (
\begin{array}{cc}
1 & 1
\end{array}
\right ) .
\end{equation}

The key point is that multiplication still works if we pretend that
those submatrices are numbers!  For example, pretending like that would
give the relation

\begin{equation}
C_{11} = A_{11} B_{11} + A_{12} B_{21}, 
\end{equation}

which the reader should verify really is correct as matrices, i.e. the
computation on the right side really does yield a matrix equal to $C_{11}$.

\subsection{Important Special Case:  Matrix Times Vector}

Consider the product of a matrix and a vector, $Ax$.  This product is a
linear combination of the columns of $A$.  To see this, write

\begin{equation}
\label{amatrix}
A = (A_1, A_2, A_3)
\end{equation}

with $A_i$ being the $i^{th}$ column, and

\begin{equation}
x = 
\left ( 
\begin{array}{c}
x_1 \\
x_2 \\
x_3
\end{array}
\right )
\end{equation}

The point is that (\ref{amatrix}) looks like a row vector --- it isn't,
but we pretend it is --- so that $Ax$ looks like the ``dot product of
one vector with another.  That would give us

\begin{equation}
\label{ax}
Ax = x_1 A_1 + x_2 A_2 + x_3 A_3
\end{equation}

As noted, we then ``unpretend,'' and find that (\ref{ax}) says that

\begin{quote}
$Ax$ is a linear combination of the columns of $A$.  The coefficients in
that linear combination are the elements of $x$.
\end{quote}

Similarly:

\begin{quote}
Let $y$ be a row vector of length equal to the number of rows of $A$.
Then $yA$ is a linear combination of the rows of A, with the
coefficients being the elements of $y$.
\end{quote}

Taking this last point a step further:

\begin{quote}

Each row of $AB$ is a linear combination of the rows of $B$, with the
coefficients for a given $B$ row being the elements of the 
corresponding row of $A$.

Similarly, each column of $AB$ is a linear combination of the columns of
$A$, with the coefficients being the elements of the corresponding
column of $B$.

\end{quote}

\subsubsection{Application of Partitioning:  Rank of a Matrix Product}

Say we have conformable matrices $A$ and $B$, and set $C = AB$.  Using
the notation $rk()$ to mean ``rank of,'' we have:

\begin{itemize}

\item The quantity $rk(C)$ is the maximal number of linearly independent
rows of $C$, among \textit{all} possible coefficients.

\item The quantity $rk(B)$ is the maximal number of linearly independent
rows of $B$, among \textit{all} possible coefficients.

\item Each row of $C = AB$ is a linear combination of rows of $B$,
i.e.\ each row of $C$ is \textit{some} linear combination of the rows of
$B$.

\item Thus we see that 

\begin{equation}
rk(B) \geq rk(C)
\end{equation}

Similarly,

\begin{equation}
rk(A) \geq rk(C)
\end{equation}

\item In other words,

\begin{equation}
rk(C) \leq min(rk(A),rk(B))
\end{equation}

\end{itemize} 

\subsection{Application of Partitioning:  Approximate Matrix Factorization}
\label{mfact}

Recall the relation 

\begin{equation}
A \approx WH
\end{equation}

in Section \ref{mf}, where $A$ is $r \times s$, $W$ is $r \times m$ and
$H$ is $m \times s$.  

The material in the last section then says:

\begin{itemize}

\item Row $i$ of $A$ is \textit{approximately} equal to a linear
combination of the rows of $H$.

\item Column $j$ of $A$ is \textit{approximately} equal to a linear
combination of the rows of $W$.

\end{itemize} 

We'll see in Chapter \ref{chap:svd} that this has big implications for
the matrix factorization method of RS.  We'll see that in a sense, $W$
will contain information about ``typical'' users, and $H$ will contain
information about ``typical'' items.

\section{Vector Norms}
\label{vecnorms}

In math, the $l_p$ \textit{norm} of a vector in $n$-dimensional space is
defined by

\begin{equation}
||x||_p = \left ( \sum_{i=1}^n [x_i|^p \right )^{1/p}
\end{equation}

This is actually a family of norms, for $1 \leq p \leq
\infty$.\footnote{$||x||_{\infty} = \max_i |x_i|$} You are familiar with
the Euclidean norm, $p = 2$.  In statistics and machine learning, we are
often minimizing the norm of some vector, usually with either $p = 1$ or
$p = 2$.

You will often see the notation $L_p$ instead of $l_p$.  However, in
math the former is used for function spaces, while the latter designates
$n$-dimensional vectors, and we use the latter here.

You will also see references to the {\it Frobenius} norm of an $r \times
s$ matrix.  That is actually just the $l_2$ norm, treating the matrix
as a length-$rs$ vector.

\section{Handy Facts}

\begin{itemize}

\item For conformable matrices $A$ and $B$, 

\begin{equation}
(AB)' = B'A'
\end{equation}

\item For invertible and conformable matrices $A$ and $B$, 

\begin{equation}
(AB)^{-1} = B^{-1} A^{-1}
\end{equation}

\item The R functions \textbf{t()} and \textbf{solve()} find the
transpose and inverse of their matrix arguments.  A more numerically
stable function for inversion is \textbf{qr()}.
\end{itemize} 
