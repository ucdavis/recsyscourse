\chapter{Statistics}
\label{chap:stat}

% move the ML/MM stuff here, with ex of a random effects model of mean
% ratings within users

% then the other ML/MM leave in the stat recysys chapter

It is assumed here that the reader has background in basic statistical
inference, i.e.\ hypothesis testing and confidence intervals.

\section{Sample vs.\ Population}

Most readers have probably noticed that when the results of a survey are
released, say during elections, a \textit{margin of error} (MOE) is stated.
For instance, ``55\% of those surveyed say they plan to vote for
Candidate Jones, with a margin of error of 3.2\%.''  The MOE is
recognition of the fact that only a sample of voters were surveyed, not
the entire population of voters.

Let $p$ denote the population proportion, i.e.\ the proportion of voters
across the population who favor Jones.  The value of $p$ is unknown, but
our estimate is $\widehat{p} = 0.55$.

Assuming the voters were polled at random, $\widehat{p}$ is a random
variable.  As such, it has a standard deviation, which can be shown to
be

\begin{equation}
[p(1-p)/n] ^{0.5}
\end{equation}

which we in turn estimate as

\begin{equation}
[\widehat{p}(1-\widehat{p})/n] ^{0.5}
\end{equation}

where $n$ is th number of people polled.  This is the \textit{standard
error} of $\widehat{p}$, and is a measure of how accurate $\widehat{p}$
is as an estimate of $p$.

What about the MOE?  This is 1.96 times the standard error, and is the
radius of an approximate 95\% confidence interval for $p$.

Every ML method is an estimator in some form or other.  However,
the terms \textit{sample} and \textit{population} are not used in the ML
community.  Instead, they speak a \textit{generative process} to mean
the same thing as sampling data from a population.

\section{How Do We Select an Estimator?}

\subsection{Sample Analogs}

In our survey example above, our estimate $\widehat{p}$ is the sample
analog of the population value $p$; the former is the proportion in our
sample data and the latter is the proportion in the population.

Similarly, say we wish to estimate a population variance, say the
variance of blood pressure $Var(B)$ in some patient population.  The
sample analog is

\begin{equation}
\label{s2}
s^2 = \frac{1}{n} \sum_{i=1}^n (B_i - \overline{B})^2
\end{equation}

where the $B_i$ are the blood pressures in our sample, and $\overline{B}$
is the average pressure in our sample.  This is analogous to the
population value,

\begin{equation}
Var(B) = E[(B - EB)^]
\end{equation}

How about estimating a covariance matrix?  Again, an extension of
(\ref{s2}).  Say our sample consists of $n$ vectors
$U_1,...,U_n$.  Then

\begin{equation}
\widehat{Cov}(U) =
\frac{1}{n}
\sum_{i=1}^n (U_i - \overline{U}) (U_i - \overline{U})'
\end{equation}

where here $\overline{U}$ is the average of the \textit{vectors} $U_i$.

In many cases, the choice of estimator is less straightforward.  For
instance, say we wish to estimate the probability density of $B$, 
$f_B()$.  Actually, you already know such an estimator---a histogram!
As long as one scales to histogram to have total area 1.0 (in R's
\lstinline{hist()} function, set \lstinline{probability=TRUE}), this
provides an estimate of $f_B()$.  Let's call the histogram
$\widehat{f}_B()$.

\subsection{Consistency of Estimators}

What do we mean in saying that an estimator is an estimate of something?
At the very least, we should require \textit{consistent}, meaning that
the estimator converges to the population quantity as the sample size
grows.

For example, in the survey example, we have 

\begin{equation}
\lim_{n \rightarrow \infty} \widehat{p} = p
\end{equation}

But what about the histogram case?  For the estimator to be consistent,
we need not only $n \rightarrow \infty$ but also $h \rightarrow 0$,
where $h$ is the bin width.\footnote{Technically, we cannot allow $h$
to go to 0 ``too quickly,'' but this issue is beyond the scope of this
book.}

\subsection{Unbiasedness of Estimators}

Another classical criterion for goodness of an estimator is that it be
\textit{unbiasded}, which means the expected value of the estimator is
the population quantity.  In the survey example above, this criterion
would ask that

\begin{equation}
E\widehat{p} = p
\end{equation}

which in fact can be shown to be true.

But $s^2$ is more complicated.  Books typically divide by $n-1$ instead
of $n$ in (\ref{s2}), as a ``fudge factor'' to make

\begin{equation}
E(s^2) = \sigma^2
\end{equation}

where the right-hand side denotes the population variance.  But actually 
even with the fudge factor $s$ (without the power 2) is biased:

\begin{equation}
0 < Var(s) = E(s^2) - (Es)^2 = \sigma^2  - (Es)^2 
\end{equation}

so

\begin{equation}
Es < \sigma
\end{equation}

rather than

\begin{equation}
Es = \sigma
\end{equation}

At the time basic statistics was being developed---about 100 years
ago---not much was known, so unbiasedness was considered important.
It in fact is still useful in some settings, but today it's understood 
that we should not emphasize it so much.

Indeed, a big issue in ML is the Bias-Variance Tradeoff.  In the above
context, dividing by $1/n$ instead of $1/(n-1)$ reduces variance, 
i.e.\ reduces $Var(s^2)$, which is good, at a cost of increasing bias, a
tradeoff.

Actually, it can be shown that the best mean squared error for $s^2$
comes from dividing by $1/(n+1)$.  We'll use $n$ here to keep the
analogy tight.  Of course, for large $n$, the difference is negligible.

\subsection{The Methods of Moments and Maximum Likelihood}
