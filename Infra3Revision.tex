\chapter{Some Infrastructure: Model Selection and Overfitting}  
\label{chap:overfit}  

Model selection is hard, more of an art than a science.  By far the most
vexing issue in statistics and machine learning is that of
\textit{overfitting}.  

\section{Toy Example}
\label{norelation}

Suppose we have just one predictor, and $n$ data points.  If we fit a
polynomial model of degree $n-1$, the resulting curve will pass through
all $n$ points, a ``perfect'' fit.  For instance:

\begin{lstlisting}
> x <- rnorm(6) 
> y <- rnorm(6) # unrelated to x!
> df <- data.frame(x,y) 
> df$x2 <- x^2 
> df$x3 <- x^3 
> df$x4 <- x^4 
> df$x5 <- x^5 
> df
           x          y         x2           x3
1 -1.1855131  0.2881291 1.40544120 -1.666168894
2 -1.7838769 -2.0741740 3.18221664 -5.676682627
3 -0.7124510 -0.4253678 0.50758640 -0.361630431
4  0.1676111 -0.1949265 0.02809348  0.004708779
5  1.2462926 -0.7348481 1.55324535  1.935798245
6  0.3741604  1.9521667 0.13999601  0.052380963
            x4            x5
1 1.975265e+00  -2.341702414
2 1.012650e+01 -18.064433938
3 2.576440e-01  -0.183558689
4 7.892437e-04   0.000132286
5 2.412571e+00   3.006769615
6 1.959888e-02   0.007333126
> lmo <- lm(y ~ .,data=df) 
> lmo

Call:
lm(formula = y ~ ., data = df)

Coefficients:
(Intercept)            x           x2           x3  
    -1.3127       4.7632      11.4809       0.5781  
         x4           x5  
    -6.9685      -2.4938  
> lmo$fitted.values 
         1          2          3          4          5 
 0.2881291 -2.0741740 -0.4253678 -0.1949265 -0.7348481 
         6 
 1.9521667 
> y
[1]  0.2881291 -2.0741740 -0.4253678 -0.1949265 -0.7348481
[6]  1.9521667
\end{lstlisting}

Yes, we ``predicted'' \textbf{y} perfectly, \textbf{even though there
was no relation between the response and predictor variables).}
Clearly that ``perfect fit'' is illusory, ``noise fitting.''  Our
ability to predict future cases would not be good.  This is
\textit{overfitting}.

\section{Real Example}
\label{prgengex}

\iffalse
library(regtools)
# polyreg does polynomial regression, forming the powers, 
# cross products etc. and then calling lm()
library(polyreg)  
data(prgeng)
pe <- prgeng[,c(1:4,6,5)]  # "Y" variable is assumed last column in polyFit()
head(pe)
tstidxs <- sample(1:nrow(prgeng),1000) 
petrn <- pe[-tstidxs,]
petst <- pe[tstidxs,]
for (i in 1:10) {
   pfout <- polyFit(petrn,deg=i)
   preds <- predict(pfout,petst)
   print(mean(abs(preds-petst$wageinc)))
}
\fi

Let's illustrate this on the dataset \textbf{prgeng}, assembled from the
2000 US census.  It consists of wage and other information on 20090
programmers and engineers in Silicon Valley.  This dataset is included
in \textbf{tegtools} package.  We will also use the \textbf{polyreg}
package, which fits polynomial models as we saw in Section \ref{poly}
above.\footnote{Available from \url{github.com/matloff}.}  

As usual, let's take a glance at the data:

\begin{lstlisting}
> head(prgeng)
       age educ occ sex wageinc wkswrkd
1 50.30082   13 102   2   75000      52
2 41.10139    9 101   1   12300      20
3 24.67374    9 102   2   15400      52
4 50.19951   11 100   1       0      52
5 51.18112   11 100   2     160       1
6 57.70413   11 100   1       0       0
\end{lstlisting}

Note that education, occupation and sex are categorical variables,
which R will convert to dummies for us.  Here is the code:

\begin{lstlisting}
library(regtools)
# polyreg does polynomial regression, forming the powers, 
# cross products etc. and then calling lm()
library(polyreg)
data(prgeng)
pe <- prgeng[,c(1:4,6,5)]  # "Y" variable is assumed last column in polyFit()
head(pe)
tstidxs <- sample(1:nrow(prgeng),1000) 
petrn <- pe[-tstidxs,]
petst <- pe[tstidxs,]
for (i in 1:4) {
   pfout <- polyFit(petrn,deg=i)
   preds <- predict(pfout,petst)
   print(mean(abs(preds-petst$wageinc)))
   print(length(pfout$fit$coefficients))
}
\end{lstlisting}

And the resulting Mean Absolute Prediction Errors and p Values :

\bigskip

\begin{tabular}{|r|r|r|}
\hline
degre & MAPE & p \\
\hline
1 & 24248.43 & 24 \\
\hline
2 & 23468.56 & 164 \\
\hline
3 & 24367.16 & 496 \\  
\hline
4 & 818934.9 & 1020 \\
\hline
\end{tabular}

Remember, $p$ is the number of predictors, including all the dummies.
When we have a degree 2 model, we have all the squared terms (except for
the dummies), and the cross-product terms, e.g. interaction between age
and gender.  So $p$ increase pretty rapidly with degree.

In any event, though, the effects of overfitting are clear.

\section{Bias vs.\ Variance}

Let's take a closer look, in an RS context.  Say we believe
(\ref{mfinteraction}) is a good model for the setting described in that
section, i.e.\ men becoming more liberal raters as they age but women
becoming more conservative.  If we omit the interaction term, than we
will underpredict older men and overpredict older women.  This biases
our ratings.

On the other hand, we need to worry about sampling variance.  Consider
the case of opinion polls during an election campaign, in which the goal
is to estimate $p$, the proportion of voters who will vote for Candidate
Jones.  If we use too small a sample size, say 50, our results will
probably be inaccurate.  This is due to sampling instability:  Two
pollsters, each randomly sampling 50 people, will sample different sets
of people, thus each having different values of $\widehat{p}$, their
sample estimates of $p$.  For a sample of size 50, it is likely that
their two values of $\widehat{p}$ will be substantially different from
each other, whereas if the sample size were 5000, the two estimates
would likely be close to each other.  In other words, the
\underline{variance} of $\widehat{p}$ is too high if the sample size is
just 50.\footnote{The repeatable experiment here is randomly choosing 50
people.  Each time we perform this experiment, we get a different set of
50 people, thus a different value of $\widehat{p}$.  The latter is a
random variable, and thus has a variance.}

In a parametric regression setting, increasing the number of terms
roughly means that the sampling variance of the $\widehat{\beta}_i$ will
increase.

So we have the famous \textit{bias/variance tradeoff}:  As we use more and
more terms in our regression model (predictors, polynomials, interaction
terms), the bias decreases but the variance increases.  This ``tug of
war'' between these decreasing and increasing quantities typically
yields a U-shaped curve:  As we increase the number of terms from 1,
mean absolute prediction error will at first decrease but eventually
will increase.  Once we get to the point at which it increases, we are
\textit{overfitting}.

This is particularly a problem when one has many dummy variables. For
instance, there are more than 42,000 ZIP (postal) codes in the US; to
have a dummy for each would almost certainly be overfitting.  If we have
only, say, 100,000 rows in our data, on average each ZIP code would have
only about 2 rows, hardly enough for a good estimate of the effect of
that code.

\section{Mathematical Analysis of the Bias vs.\ Variance Tradeoff}

Let's take a more precise  look, employing a simple mathematical model.

\subsection{The Setting}

Suppose we have the samples of men's and women's heights, $X_1,...,X_n$
and $Y_1,...,Y_n$.  Assume for simplicity that the population variance
of height is the same for each gender, $\sigma^2$.  The means of the two
populations are designated by $\mu_1$ and $\mu_2$.

Say we wish to guess the height of a new person who we know to be a man
but for whom we know nothing else.  We do not see him, etc.

Suppose for just a moment that we actually know the distribution of X,
i.e. the {\it population} distribution of male heights.  What would be
the best constant g to use as our guess for a person about whom we know
nothing other than gender?

It is easily shown that the mean squared error MSE 

\begin{equation}
E[(g-X)^2]
\end{equation}

is minimized by setting $g = \mu_1$.  Our best guess for this unseen
man's height is the mean height of all men in the population.  (Note
that ``mean'' above averaged over all possible men in the population.) 

Of course, we don't know $\mu_1$, but we can do the next-best thing,
i.e.\ use an estimate of it from our sample.  The natural choice for
that estimator would be

\begin{equation}
T_1 = \overline{X},
\end{equation}

the mean height of men in our sample.

\subsection{Context of Interest: Very Small Sample}

But what if our sample size $n$ is really small, say $n = 5$?  That's
awfully small.  We may wish to consider pooling the women's heights into
our estimate, in order to get a larger sample.  Then we would estimate
$\mu_1$ by incorporating the sample mean of women's heights,
$\overline{Y}$:

\begin{equation}
T_2 = \frac{\overline{X}+\overline{Y}}{2}, 
\end{equation}

It may at first seem obvious that $T_1$ is the better estimator.  Women
tend to be shorter, after all, so pooling the data from the two genders
would induce a \underline{bias}, defined as  

\begin{equation}
\textrm{bias = mean of the estimator - true population value}
\end{equation}

Here  ``mean'' refers to the average of the estimator over all possible
samples from this population.  

It can be shown that for a sample mean $M$, drawn from a population with
mean $\nu$,

\begin{equation}
E(M) = \nu
\end{equation}

In other words, $M$ has 0 bias.  Thus our $T_1$ here has 0 bias.  But
that is not the case for $T_2$:

\begin{equation}
E(T_2) = 0.5 E(\overline{X}) + 0.5 E(\overline{Y}) = (\mu_1 + \mu_2) / 2
< \mu_1
\end{equation}

In other words.  $T_2$ would have a negative bias.  

For an estimator of $T$ of some population quantity $\theta$, its
\textit{mean square error} is defined to be

\begin{equation}
MSE = E[(T - \theta)^2] 
\end{equation}

One can derive that 

\begin{equation}
\textrm{MSE = variance of the estimator} + \textrm{bias of the
estimator} ^2
\end{equation}

In other words, {\it some amount of bias may be tolerable}, if it will
buy us a subtantial reduction in variance.  After all, women are not
that much shorter than men, so the bias might not be too bad.
Meanwhile, the pooled estimate should have lower variance, as it is
based on $2n$ data points, rather than $n$.

Before continuing, note first that $T_2$ is based on a simpler model
than is $T_1$, as $T_2$ ignores gender.  We thus refer to $T_1$ as being
based on the more complex model.

So, the question becomes, which has the smaller MSE, $T_1$ or
$T_2$?  In other words:

\begin{quote}
Which is smaller, $E[(T_1 - \mu_1)^2]$ or $E[(T_2 - \mu_1)^2]$?
\end{quote}

\subsection{Drawing Conclusions from This Example}

After some elementary math stat operations, one can show that
$T_1$ is a better predictor than $T_2$ if 

\begin{equation}
\label{betteriff}
\left ( \frac{\mu_2-\mu_1}{2} \right)^2 >
\frac{\sigma^2}{2n}
\end{equation}

Granted, we don't know the values of the $\mu_1$ and $\sigma^2$, so in a
real situation, we won't really know whether to use $T_1$ or $T_2$.  But
the above analysis makes the point that under some circumstances, it
really is better to pool the data in spite of bias.

So you can see that $T_1$ is better only if either

\begin{itemize}

\item n is large enough, or

\item the difference in population mean heights between men and women is
large enough, or

\item there is not much variation within each population, e.g. most men
have very similar heights

\end{itemize}

In other words:

\begin{quote}
A more complex model is more accurate than a simpler one only if either 

\begin{itemize}

\item [(a)] we have enough data to support it, or

\item [(b)] the complex model is sufficiently different from the simpler one

\end{itemize}

\end{quote}

A very rough, intuitive way to view (a) is that our data is being
``shared'' by all the parameters to be estimated.  In our example above,
the simple model had one parameter, $\mu$ while the complex one had two,
$\mu_1$ and $\mu_2$.  Due to this ``sharing,'' each parameter in the
complex version has ``a smaller piece of the pie.''

In Section \ref{prgengex}, we ran an \textbf{lm()} model with 2624
parameters, definitely a complex model.  Was $n = 100000$ large enough
to satisfy (a) above?  We don't know, but again, it raises the issue of
possible overfitting.




\section{Can Anything Be Done about It?}
\label{dosomething}

So, where is the ``happy medium,'' the model that is rich enough to
capture most of the dynamics of the variables at hand, but simple enough
to avoid variance issues?  Unfortunately, \textbf{there is no good answer to
this question.}

\subsection{Rough Rule of Thumb}

One quick rule, backed up by mathematical theory, is that one should
have $p < \sqrt{n}$, where $p$ is the number of predictors, including
polynomial and interaction terms (not to be confused with the quantity
of the same name in our polling example above), and $n$ is the number of
cases in our sample.  But this is certainly not a firm rule by any
means, and I find it tends to be overly conservative.

\subsection{Cross-Validation}

From the polynomial-ftting example in Section \ref{norelation}, we see
the following key point:

\begin{quote}
An assessment of predictive ability, based on predicting the same data
on which our model is fit, tends to be overly optimistic and may be
meaningless or close to it.
\end{quote}

This motivates the most common approach to dealing with the
bias/variance tradeoff, \textit{cross validation}.  In the simplest
version, one randomly splits the data into a \textit{training set} and a
\textit{test set}.\footnote{The latter is also called a \textit{holdout
set} or a \textit{validation set}.  Note that there are many variants of
this, e.g. something called \textit{K-fold cross validation}.}  We fit
the model to the training set and then, pretending we don't know the
``Y'' (i.e. response) values in the test set, predict those values from
our fitted model and the ``X'' values (i.e.\ the predictors) in the test
set.  We then ``unpretend,'' and check how well those predictions
worked. 

The test set is ``fresh, new'' data, since we called \textbf{lm()} or
whatever only on the training set.  Thus we are avoiding the ``noise
fitting'' problem.  We can try several candidate models, then choose the
one that best predicts the test data.

For example, consider the analysis in Section \ref{lmex}.  Say we are
considering adding age and gender as predictors.  Call the original
model Model 1 and the one with predictors added Model 2.  We would do
the following:

\begin{itemize}

\item [(a)] Randomly partition the 100,000 rows of the data frame into
training and holdout tests of size 95,000 and 5000.

\item [(b)] Fit Model 1 to the training set, and use it on the predictor
values in the holdout set to predict the ratings in that set.  Compute
some accuracy measure, say Mean Absolute Prediction Error (MAPE).

\item [(c)] Do as in (a), but with Model 2 instead of Model 1.

\item [(d)] Compare the two MAPE values, and choose the better model.

\item [(e)] For all (user,movie) pairs with unknown ratings, use the
model from (d) to predict.

\end{itemize} 

(Note carefully that after fitting the model via cross-validation, we
then use the full data for later prediction.  Splitting the data for
cross-validation was just a temporary device for model selection.)

Cross-validation is essentially the standard for model selection, and it
works well if we only try a few models.  Problems can occur if we try
many models, as seen in the next section.

\subsection{Regularization}

Suppose we are estimating a vector mean $\mu$, using sample data on a
vector $X$.  For instance, we may have $X$ equal to
(height,weight,age,blood pressure).  Following standard notation, let $p$
denote the number of components of $X$, e.g. $p = 4$ in the above
example.

The standard estimate is of course the sample mean, $\overline{X}$.  In
the above example, this would be the 4-vector consisting of the averages
of height, weight, age and blood pressure in our sample.

Some years ago, mathematical statistician Charles Stein caused quite a
stir by proving the following remarkable fact:

\begin{itemize}

\item If $p \leq 2$, then $\overline{X}$ is the optimal estimator of
$\mu$.\footnote{Under Mean Squared Error loss.}

\item If $p \geq 3$, then the optimal estimator is $c \overline{X}$
for some $0 < c < 1$.

h\end{itemize} 

So, in higher dimensions --- remember, we are working with $p$ in the
dozens or even hundreds --- we should shrink down our estimator.  The
intuition here is this:  Occasionally sample data will contain some
really extreme data points, and these skew our $\overline{X}$ estimator.
By shrinking down the estimator, we reduce the influence of those
extreme values.  And with $p \geq 3$, extreme values happen often
enough to make shrinkage (or \textit{regularization}) a ``win.''

This was later applied to linear regression models, PCA and so on.
Instead of finding $b$ that minimizes (\ref{rss}), we minimize

\begin{equation}
r =
\sum_{i=1}^n [W_i - (b_0 + b_1 H_i + b_2 A_i)]^2 + \lambda ||b||_1
\end{equation}

where $||b||_1$ is the $l_1$ vector norm of $b$:

\begin{equation}
||b|| = \sum_{i=1}^p |b_i|
\end{equation}


This is not done directly out of concern for outliers so much as
\textbf{is as a remedy to overfitting.}  In the polynomial models we
discussed earlier, higher-degree models at least have more components in
$b$ but also tend to be larger due to high variance.  Of course, we have
to choose $\lambda$, a tuning parameter (as $s$ was for PCA); this is
typically done by trying various values and assessing via
cross-validation.

This technique in linear regression is called the \textit{LASSO},
the Least Absolute Shrinkage and Selection Operator.  A popular
implementation in R is the \textbf{lars} package.

\section{The Problem of p-Hacking}

The (rather recent) term \textit{p-hacking} refers to the following
abuse of statistics.\footnote{The term \textit{abuse} here will not
necessarily connote intent. It may occur out of ignorance of the
problem.}

Say we have 250 pennies, and we wish to determine whether any are
unbalanced, i.e.\ have probability of heads different from 0.5.  We do
so by tossing each coin 100 times.  If we get fewer than 40 heads or
more than 60, we decide this coin is unbalanced.\footnote{For those who
know statistics:  This gives us a Type I error rate of about 0.05, the
standard used by most people.}  The problem is that, even if all the
coins are perfectly balanced, we eventually will have one that has fewer
than 40 or greater than 60 heads, just by accident.  \textbf{We will
then \underline{falsely} declare this coin to be unbalanced.}

Or, to give a somewhat frivolous example that still will make the point,
say we are investigating whether there is any genetic component to a
person's sense of humor.  Is there a Humor gene?  There are many, many
genes to consider.  Testing each one for relation to sense of humor is
like checking each penny for being unbalanced: Even if there is no Humor
gene, then eventually, just by accident, we'll stumble upon one that
seems to be related to humor.\footnote{For those with background in
statistics, the reason this is called ``p-hacking'' is that the
researcher may form a significance test for each gene, computing a
p-value for each test.  Since under the null hypothesis we have a 5\%
chance of getting a ``significant'' p-value for any given gene, the
probability of having at least one significant result out of the
thousands of tests is quite high, even if the null hypothesis is true in
all cases.  There are techniques called \textit{multiple inference} or
\textit{multiple comparison} methods, to avoid p-hacking in performing
statistical inference.  See for example \textit{Multiple Comparisons:
Theory and Methods}, Jason Hsu, 1996, CRC.}

Though the above is not about prediction, it has big implications for
the prediction realm.  In ML there are various datasets on which
analysts engage in contests, vying for the honor of developing the model
with the highest prediction accuracy, say for classification of images.
If there is a large number of analysts competing for the prize, then
even if all the analysts have models of equal accuracy, it is likely
that one is substantially higher than the others, just due to an
accident of sampling variation.  This is true in spite of the fact that
they all are using the same sample; it may be that the ``winning''
analyst' model happens to do especially well in the given data, and may
not be so good on another sample from the same population.  So, when some
researcher sets a new record on a famous ML dataset, it may be that the
research really has found a better prediction model --- or it may be
that it merely looks better, due to p-hacking.

The same is true for your own analyses.  If you try a large number of
models, the ``winning'' one may actually not be better than all the
others.

This also implies that cross-validation is no panacea either.  If we
compare a large number of models, there is a danger that one looks
really good when it is not.

\section{A Note on Covariates}

Covariates can substantially enhance prediction power for users or items
for whom we have rather little data, but their impact otherwise may be
small.\footnote{This rules out the MovieLens data, for instance, as its
curator omitted users who had rated fewer than 25 movies.}  The
intuition underlying this is that if we have a lot of ratings from some
user, they already tell us lots about her.  Covariate information
about this user may not add much to what we already know.\footnote{Note
that with a linear model, we could not use the covariate anyway, as
explained in Section \ref{lindep}.}

Consider for instance the House Voting dataset at
UCI.\footnote{https://archive.ics.uci.edu/ml/datasets/congressional+voting+records}
For each member of the US House of Representatives, we have their votes,
Yes or No, on some bills.  One can treat this as a ``recommender
system,'' with the congresspeople as users and bills as items.  We can
thus try to predict how a given member of Congress would have voted on
some bill on which she did not actually vote.

The data also tell us whether the politician is a Democrat or
Republican, so party affiliation is a covariate.  But if she has voted
on many bills, that record already gives us a good idea as to her party,
and thus this covariate may not be very helpful.

On the other hand, another possible covariate would be the congressional
district that this person represents.  If it is one that is heavily
agricultural, for instance, she would likely support a farm bill,
regardless of party.  Note that in that case, the type of bill is a
covariate for the item, so both covariates could be important.

