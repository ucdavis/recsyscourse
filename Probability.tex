\chapter{Probability}

% take from 132: 2-D prob mass ftn, marble ex, as generaliz of 1-D;
% covarince as extensive of variance in 1-D; correl is scaled version;
% define in terms of rho, sigmas; MovieLens ex!!!!; age, userMean,
% Nuser; rho in [-1,1] by Cauchy-Schwartz (optional)

It is assumed the reader has background in calculus-based probability
constructs, e.g.\ density functions and distributions involving
infinite series.  Here we treat some advanced topics used in the sequel.

\section{Multivariate Distributions}

\subsection{Multivariate Probability Mass Functions}
\label{marblepmf} 

Recall that for a single discrete random variable $X$, the distribution of
$X$ is defined to be a list of all the values of $X$, together with the
probabilities of those values.  We encapsulate those in the
\textit{probability mass function}:

\begin{equation}
p_X(i) = P(X = i)
\end{equation}

Here the argument is $i$.

This is extended to a pair of discrete random variables $U$ and $V$ as

\begin{equation}
p_{U,V}(i,j) = P(U = i, V = j)
\end{equation}

with arguments $i$ and $j$.

For example, suppose we have a bag containing two yellow marbles, three
blue ones and four green ones.  We choose four marbles from the bag at
random, without replacement.  Let $Y$ and $B$ denote the number of yellow
and blue marbles that we get.  Then

\begin{equation}
p_{Y,B}(i,j) = P(Y = i \textrm{ and } B = j) = 
\frac
{
\binom{2}{i}
\binom{3}{j}
\binom{4}{4-i-j}
}
{\binom{9}{4}}
\end{equation}

Here is a table displaying all the values of P(Y = i and B = j):

\begin{tabular}{|r|r|r|r|r|r|}
\hline
i $\downarrow$, j $\rightarrow$ & 0 & 1 & 2 & 3 \\ \hline
0 & 0.0079 & 0.0952 & 0.1429 & 0.0317 \\ \hline
1 & 0.0635 & 0.2857 & 0.1905 & 0.1587 \\ \hline
2 & 0.0476 & 0.0952 & 0.0238 & 0.000 \\ \hline
\end{tabular}

So in our marble example above, $p_{Y,B}(1,2) = 0.048$, $p_{Y,B}(2,0) =
0.012$ and so on.

\subsection{Multivariate Density Functions}

Recall that for a continuous random variable $X$, we can find
probabilities involving $X$ by integrating its density:

\begin{equation}
P(X \textrm{ in } A) = \int_{A} f_X(t) ~ dt
\end{equation}

for regions $A$ in the real line.

This extends to multiple dimensions.  For sets $A$ in the plane,

\begin{equation}
P[(X,Y) \textrm{ in } A] = \int_{A} \int f_{X,Y}(s,t) ~ ds  ~ dt
\end{equation}

\section{Relations Between Variables}

\subsection{Covariance}

You have seen the \textit{variance} of a single random variable before,

\begin{equation}
Var(X) = E[(X - EX)^2]
\end{equation}

How does this extend?  The answer is the \textit{covariance}:

\begin{equation}
Cov(X,Y) = E[(X - EX) (Y - EY)]
\end{equation}

Think of height and weight in a human population, $X$ and $Y$. People
who are taller than average tend to be heavier than average, making

\begin{equation}
(X - EX) (Y - EY) > 0
\end{equation}

Shorter people tend to be lighter than average, but again

\begin{equation}
(X - EX) (Y - EY) > 0
\end{equation}

Now, 

\begin{equation}
E[(X - EX) (Y - EY)]
\end{equation}

is the average value of that product, as we range through various people
in the population, so the covariance will be positive.  Note that 
$(X - EX) (Y - EY)$ won't be positive for all people, but if the
relation is typical enough, the average will be positive.

Note that $Cov(X,X) = Var(X)$.

\section{Covariance Matrices}

Say we have $p$ random variables $X_1$,..., $X_p$.  Their
\textit{covariance matrix} is $p \times p$, with the $(i,j)$
element being $Cov(X_i,X_j)$.  The diagonal elements are the variances
of the individual variables.

\subsection{Correlation}

In other words, roughly speaking, positively-related variable will have
positive covariance, with a similar statement for negatively-related
variables.  In fact, correlation is just scaled-down variance:

\begin{equation}
\rho(X,Y) = 
\frac{Cov(X,Y)}{\sigma(X) ~ \sigma(Y)}
\end{equation}

where $\rho$ and $\sigma$ are standard symbols for correlation and
standard deviation, respectively.

One can show that 

\begin{equation}
\label{rho1}
-1 \leq \rho \leq 1
\end{equation}

In fact, one way to show this is to make a vector space of random
variables, and use the Cauchy-Schwarz Inequality!  Specifically, 
let $\mathcal{W}$ be the set of all random variables with finite
variance.  It is clearly closed under linear combinations, and we can
define an inner product (``dot product'') by

\begin{equation}
<U,V> = Cov(U,V)
\end{equation}

The vector norm will be

\begin{equation}
||U|| = (<U,U>)^{0.5} = \sigma(U)
\end{equation}

The Cauchy-Schwarz Inequality says that the absolute value of the inner
product of two vectors, divided by the product of their norms, is at
most 1.  That gives us (\ref{rho1})!

So, is a correlation of, say, 0.4, large or small?  It is customary to
gauge this by the squared correlation, which arises in the theory of
linear predictors (Chapter \ref{chap:ml}).  The quantity $\rho^2$ can be
shown to be the reduction in mean squared prediction error when we
predict $V$ using $U$ (or vice versa).

\subsection{Example:  MovieLens}

Here is a glimpse of a dataset we'll cover in detail later on, MovieLens
plus some side information:

\begin{lstlisting}
 head(ml100kpluscovs)
  user item rating timestamp age gender        occ   zip userMean Nuser G1 G2
1    1    1      5 874965758  24      M technician 85711 3.610294   272  0  0
2  117    1      4 880126083  20      M    student 16125 3.918605    86  0  0
3  429    1      3 882385785  27      M    student 29205 3.393720   414  0  0
4  919    1      4 875289321  25      M      other 14216 3.470046   217  0  0
5  457    1      4 882393244  33      F   salesman 30011 4.025271   277  0  0
6  468    1      5 875280395  28      M   engineer 02341 3.993007   143  0  0
  G3 G4 G5 G6 G7 G8 G9 G10 G11 G12 G13 G14 G15 G16 G17 G18 G19 itemMean Nitem
1  0  1  1  1  0  0  0   0   0   0   0   0   0   0   0   0   0 3.878319   452
2  0  1  1  1  0  0  0   0   0   0   0   0   0   0   0   0   0 3.878319   452
3  0  1  1  1  0  0  0   0   0   0   0   0   0   0   0   0   0 3.878319   452
4  0  1  1  1  0  0  0   0   0   0   0   0   0   0   0   0   0 3.878319   452
5  0  1  1  1  0  0  0   0   0   0   0   0   0   0   0   0   0 3.878319   452
6  0  1  1  1  0  0  0   0   0   0   0   0   0   0   0   0   0 3.878319   452
\end{lstlisting}

(The G's are genres.)

Let's find the correlations:

\begin{lstlisting}
> cor(ml100kpluscovs[,c('age','userMean','Nuser')])
                 age   userMean       Nuser
age       1.00000000  0.1355457 -0.03908845
userMean  0.13554566  1.0000000 -0.30755257
Nuser    -0.03908845 -0.3075526  1.00000000
\end{lstlisting}

There seems to be rather little relation betwen age and the mean rating
for a user, and between age and the number of ratings for a user.  But
the latter two variables seem to have a negative relation (still rather
weak, as other theory shows we should square correlations); users who
give higher ratings rate fewer movies.

\subsection{An Important Matrix Property}

Say we have a random \textit{vector} $X$, length $p$, and a $k \times p$
nonrandom matrix $A$.  Then $AX$ is a new random vector, of length $k$.
One can show that

\begin{equation}
Cov(AX) = A ~ Cov(X) ~ A'
\end{equation}

where here $Cov()$ with a single argument means covariance matrix, and
$'$ denotes transpose.

